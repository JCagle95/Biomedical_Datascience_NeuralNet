{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Neural Network for Keras Sample Datasets\n",
    "The first section will follow the instruction of Jason Browniee's 2016 post on Neural Network with Keras. The original post is here: http://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "\n",
    "The only modification I made is the input size to match the dataset we had and to split the training set further into training and validation instead of using testing data for validation in Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "seed = 78\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow the Blog to do CNN and MLP for Digits Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, input_dim=num_pixels, kernel_initializer='normal', activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_feat, X_train_val, y_train_feat, y_train_val = train_test_split(X_train, y_train, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "5s - loss: 0.2965 - acc: 0.9157 - val_loss: 0.1437 - val_acc: 0.9597\n",
      "Epoch 2/10\n",
      "5s - loss: 0.1184 - acc: 0.9663 - val_loss: 0.1006 - val_acc: 0.9715\n",
      "Epoch 3/10\n",
      "5s - loss: 0.0783 - acc: 0.9772 - val_loss: 0.0887 - val_acc: 0.9735\n",
      "Epoch 4/10\n",
      "5s - loss: 0.0546 - acc: 0.9845 - val_loss: 0.0683 - val_acc: 0.9780\n",
      "Epoch 5/10\n",
      "5s - loss: 0.0395 - acc: 0.9885 - val_loss: 0.0699 - val_acc: 0.9793\n",
      "Epoch 6/10\n",
      "5s - loss: 0.0289 - acc: 0.9921 - val_loss: 0.0595 - val_acc: 0.9817\n",
      "Epoch 7/10\n",
      "5s - loss: 0.0215 - acc: 0.9945 - val_loss: 0.0600 - val_acc: 0.9807\n",
      "Epoch 8/10\n",
      "5s - loss: 0.0162 - acc: 0.9964 - val_loss: 0.0613 - val_acc: 0.9808\n",
      "Epoch 9/10\n",
      "5s - loss: 0.0115 - acc: 0.9976 - val_loss: 0.0597 - val_acc: 0.9828\n",
      "Epoch 10/10\n",
      "5s - loss: 0.0090 - acc: 0.9982 - val_loss: 0.0567 - val_acc: 0.9837\n",
      "Digits: MLP Accuracy: 98.10%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = baseline_model()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_feat, y_train_feat, validation_data=(X_train_val, y_train_val), epochs=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Digits: MLP Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Convolutional_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_feat, X_train_val, y_train_feat, y_train_val = train_test_split(X_train, y_train, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "5s - loss: 0.2891 - acc: 0.9177 - val_loss: 0.1413 - val_acc: 0.9598\n",
      "Epoch 2/10\n",
      "5s - loss: 0.1167 - acc: 0.9665 - val_loss: 0.1022 - val_acc: 0.9700\n",
      "Epoch 3/10\n",
      "5s - loss: 0.0751 - acc: 0.9789 - val_loss: 0.0795 - val_acc: 0.9775\n",
      "Epoch 4/10\n",
      "5s - loss: 0.0535 - acc: 0.9844 - val_loss: 0.0712 - val_acc: 0.9787\n",
      "Epoch 5/10\n",
      "6s - loss: 0.0387 - acc: 0.9889 - val_loss: 0.0725 - val_acc: 0.9797\n",
      "Epoch 6/10\n",
      "5s - loss: 0.0294 - acc: 0.9918 - val_loss: 0.0632 - val_acc: 0.9823\n",
      "Epoch 7/10\n",
      "5s - loss: 0.0205 - acc: 0.9952 - val_loss: 0.0663 - val_acc: 0.9822\n",
      "Epoch 8/10\n",
      "5s - loss: 0.0157 - acc: 0.9964 - val_loss: 0.0592 - val_acc: 0.9840\n",
      "Epoch 9/10\n",
      "5s - loss: 0.0113 - acc: 0.9978 - val_loss: 0.0595 - val_acc: 0.9835\n",
      "Epoch 10/10\n",
      "5s - loss: 0.0084 - acc: 0.9985 - val_loss: 0.0649 - val_acc: 0.9825\n",
      "Digits: Convolutional Neural Network Error: 97.99%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train_feat, y_train_feat, validation_data=(X_train_val, y_train_val), epochs=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Digits: Convolutional Neural Network Error: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat for CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of a Multilayer Perceptron\n",
    "The input to multilayer perceptron must be a vector of input. Since our input are 50000 x 32 x 32 x 3, we need to convert that to 50000 x 3072 vector.\n",
    "\n",
    "The MLP is 1 input layer with 3072 input node, 1 hidden layer with 3072 input node, and 1 output layer with 1000 output node\n",
    "\n",
    "The output will be converted for multiclass classification using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_pixels = X_train.shape[1]*X_train.shape[2]*X_train.shape[3]\n",
    "X_trainVec = X_train.reshape(X_train.shape[0],num_pixels).astype('float32')\n",
    "X_testVec = X_test.reshape(X_test.shape[0],num_pixels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler = scaler.fit(X_trainVec)\n",
    "X_trainVec = scaler.transform(X_trainVec)\n",
    "X_testVec = scaler.transform(X_testVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, input_dim=num_pixels, kernel_initializer='normal', activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_feat, X_train_val, y_train_feat, y_train_val = train_test_split(X_trainVec, y_train, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "47s - loss: 13.0141 - acc: 0.1588 - val_loss: 13.1327 - val_acc: 0.1682\n",
      "Epoch 2/10\n",
      "50s - loss: 13.1494 - acc: 0.1650 - val_loss: 13.0881 - val_acc: 0.1670\n",
      "Epoch 3/10\n",
      "52s - loss: 13.0685 - acc: 0.1697 - val_loss: 13.0675 - val_acc: 0.1596\n",
      "Epoch 4/10\n",
      "184s - loss: 13.0928 - acc: 0.1669 - val_loss: 13.2414 - val_acc: 0.1484\n",
      "Epoch 5/10\n",
      "149s - loss: 13.0291 - acc: 0.1719 - val_loss: 12.9829 - val_acc: 0.1710\n",
      "Epoch 6/10\n",
      "149s - loss: 12.9736 - acc: 0.1753 - val_loss: 12.9443 - val_acc: 0.1770\n",
      "Epoch 7/10\n",
      "150s - loss: 12.9884 - acc: 0.1718 - val_loss: 12.9383 - val_acc: 0.1796\n",
      "Epoch 8/10\n",
      "154s - loss: 12.9684 - acc: 0.1752 - val_loss: 12.9485 - val_acc: 0.1750\n",
      "Epoch 9/10\n",
      "152s - loss: 12.9701 - acc: 0.1745 - val_loss: 12.9486 - val_acc: 0.1762\n",
      "Epoch 10/10\n",
      "154s - loss: 12.9639 - acc: 0.1758 - val_loss: 12.9688 - val_acc: 0.1716\n",
      "Multilayer Perceptron Accuracy: 16.84%\n"
     ]
    }
   ],
   "source": [
    "model = baseline_model()\n",
    "model.fit(X_train_feat, y_train_feat, validation_data=(X_train_val, y_train_val), epochs=10, batch_size=200, verbose=2)\n",
    "scores = model.evaluate(X_testVec, y_test, verbose=0)\n",
    "print(\"Multilayer Perceptron Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of a Simple Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# Load Dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3]).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], X_train.shape[1], X_test.shape[2], X_test.shape[3]).astype('float32')\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convNN():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, (3, 3), padding='valid', input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3]), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_feat, X_train_val, y_train_feat, y_train_val = train_test_split(X_train, y_train, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "48s - loss: 1.7889 - acc: 0.3558 - val_loss: 1.5320 - val_acc: 0.4536\n",
      "Epoch 2/10\n",
      "49s - loss: 1.4267 - acc: 0.4931 - val_loss: 1.3599 - val_acc: 0.5158\n",
      "Epoch 3/10\n",
      "49s - loss: 1.3038 - acc: 0.5430 - val_loss: 1.2819 - val_acc: 0.5414\n",
      "Epoch 4/10\n",
      "65s - loss: 1.2339 - acc: 0.5699 - val_loss: 1.2214 - val_acc: 0.5708\n",
      "Epoch 5/10\n",
      "62s - loss: 1.1916 - acc: 0.5857 - val_loss: 1.1868 - val_acc: 0.5882\n",
      "Epoch 6/10\n",
      "62s - loss: 1.1549 - acc: 0.5987 - val_loss: 1.1573 - val_acc: 0.5934\n",
      "Epoch 7/10\n",
      "62s - loss: 1.1138 - acc: 0.6130 - val_loss: 1.1433 - val_acc: 0.5998\n",
      "Epoch 8/10\n",
      "62s - loss: 1.0835 - acc: 0.6243 - val_loss: 1.1167 - val_acc: 0.6132\n",
      "Epoch 9/10\n",
      "63s - loss: 1.0575 - acc: 0.6316 - val_loss: 1.1047 - val_acc: 0.6174\n",
      "Epoch 10/10\n",
      "63s - loss: 1.0356 - acc: 0.6388 - val_loss: 1.1215 - val_acc: 0.6024\n",
      "Convolutional Neural Network Accuracy: 60.09%\n"
     ]
    }
   ],
   "source": [
    "model = convNN()\n",
    "model.fit(X_train_feat, y_train_feat, validation_data=(X_train_val, y_train_val), epochs=10, batch_size=200, verbose=2)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Convolutional Neural Network Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
